{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Installing Library\n",
    "%pip install selenium --quiet\n",
    "%pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd \n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver(url):\n",
    "    #Return web driver\n",
    "    colab_options = webdriver.ChromeOptions()\n",
    "    colab_options.add_argument('--no-sandbox')\n",
    "    colab_options.add_argument('--disable-dev-shm-usage')\n",
    "    colab_options.add_argument('--headless')\n",
    "    colab_options.add_argument('--start-maximized') \n",
    "    colab_options.add_argument('--start-fullscreen')\n",
    "    colab_options.add_argument('--single-process')\n",
    "    driver = webdriver.Chrome(options=colab_options)\n",
    "    driver.get(url)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_header(driver):\n",
    "    # Return Table columns in list form\n",
    "    header = driver.find_elements(By.TAG_NAME, value= 'th')\n",
    "    header_list = [item.text for index, item in enumerate(header) if index < 10]\n",
    "    return header_list\n",
    "\n",
    "def get_table_rows(driver):\n",
    "    # Get number of rows available\n",
    "    tablerows = len(driver.find_elements(By.XPATH, value='//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr'))\n",
    "    return tablerows  \n",
    "\n",
    "def parse_table_rows(rownum, driver, header_list):\n",
    "    # Loop for each row to get the data and return column value in the form of dictionary\n",
    "    row_dictionary = {}\n",
    "    for index , item in enumerate(header_list):\n",
    "        time.sleep(1/20)\n",
    "        column_xpath = '//*[@id=\"scr-res-table\"]/div[1]/table/tbody/tr[{}]/td[{}]'.format(rownum, index+1)\n",
    "        row_dictionary[item] = driver.find_element(By.XPATH, value=column_xpath).text\n",
    "    return row_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Data \n",
    "def parse_multiple_pages(driver, total_crypto):\n",
    "    # Loop through each row, perform button click to move to another page\n",
    "    table_data = []\n",
    "    page_num = 1\n",
    "    is_scraping = True\n",
    "    header_list = get_table_header(driver)\n",
    "\n",
    "    while is_scraping:\n",
    "        table_rows = get_table_rows(driver)\n",
    "        print('Found {} rows on Page : {}'.format(table_rows, page_num))\n",
    "        print('Parsing Page : {}'.format(page_num))\n",
    "        table_data += [parse_table_rows(i, driver, header_list) for i in range (1, table_rows + 1)]\n",
    "        total_count = len(table_data)\n",
    "        print('Total rows scraped : {}'.format(total_count))\n",
    "        if total_count >= total_crypto:\n",
    "            print('Done Parsing...')\n",
    "            is_scraping = False\n",
    "        else:    \n",
    "            print('== Moving to Next Page ==')\n",
    "            element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"scr-res-table\"]/div[2]/button[3]')))\n",
    "            element.click() \n",
    "            page_num += 1\n",
    "    return table_data\n",
    "\n",
    "# Converting Large Number Data\n",
    "def convert(attribute):\n",
    "    #Convert attribute value from string format to float format based on the suffix (T, B, M)\n",
    "    result = None\n",
    "    \n",
    "    if attribute.endswith('T'):\n",
    "        result = float(attribute[:-1]) * 1e6\n",
    "    elif attribute.endswith('B'):\n",
    "        result = float(attribute[:-1]) * 1e3\n",
    "    elif attribute.endswith('M'):\n",
    "        result = float(attribute[:-1]) * 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Data Cleaning\n",
    "def clean_data(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        # Clean the 'Market Cap' and 'Volume' attribute\n",
    "        market_cap = item[\"Market Cap\"].replace(',', '')\n",
    "        market_cap = convert(market_cap)\n",
    "\n",
    "        volume = item[\"Total Volume All Currencies (24Hr)\"].replace(',', '')\n",
    "        volume = convert(volume)\n",
    "\n",
    "        circ_supply = item[\"Circulating Supply\"].replace(',', '')\n",
    "        circ_supply = convert(circ_supply)\n",
    "\n",
    "        # Clean the data item and add it to the cleaned_data list\n",
    "        cleaned_item = {\n",
    "            \"Symbol\": item[\"Symbol\"].strip(),\n",
    "            \"Name\": item[\"Name\"].strip(),\n",
    "            \"Price (Intraday)\": float(item[\"Price (Intraday)\"].replace(',', '')),\n",
    "            \"Change\": float(item[\"Change\"].replace(',', '')),\n",
    "            \"% Change\": float(item[\"% Change\"].replace('%', '')),\n",
    "            \"Market Cap\": market_cap,\n",
    "            \"Volume in Currency (Since 0:00 UTC)\": (item[\"Volume in Currency (Since 0:00 UTC)\"].replace(',', '')),\n",
    "            \"Volume in Currency (24Hr)\": (item[\"Volume in Currency (24Hr)\"].replace(',', '')),\n",
    "            \"Total Volume All Currencies (24Hr)\": volume,\n",
    "            \"Circulating Supply\": circ_supply\n",
    "        }\n",
    "        cleaned_data.append(cleaned_item)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Export\n",
    "def scrape_data(url, total_crypto):\n",
    "    # Scrape Yahoo Finance cryptocurrency data and separate it into different JSON files based on entities\n",
    "    print('Creating driver')\n",
    "    driver = get_driver(url)\n",
    "    print(\"Web Title: \", driver.title)\n",
    "    print(\"The web scraped at:\", datetime.datetime.now())\n",
    "    table_data = parse_multiple_pages(driver, total_crypto)\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "\n",
    "    # Clean the scraped data\n",
    "    cleaned_data = clean_data(table_data)\n",
    "\n",
    "    # Separate the data into different dictionaries based on entities\n",
    "    cryptocurrency_data = []\n",
    "    price_data = []\n",
    "    volume_data = []\n",
    "\n",
    "    unique_symbols = set()  # Set to store unique symbols\n",
    "\n",
    "    for item in cleaned_data:\n",
    "        symbol = item[\"Symbol\"]\n",
    "\n",
    "        # Skip duplicate symbols\n",
    "        if symbol in unique_symbols:\n",
    "            continue\n",
    "\n",
    "        unique_symbols.add(symbol)\n",
    "\n",
    "        cryptocurrency_item = {\n",
    "            \"Symbol\": symbol,\n",
    "            \"Name\": item[\"Name\"],\n",
    "            \"Market Cap (in M)\": item[\"Market Cap\"],\n",
    "            \"Circulating Supply (in M)\": item[\"Circulating Supply\"]\n",
    "        }\n",
    "        cryptocurrency_data.append(cryptocurrency_item)\n",
    "\n",
    "        price_item = {\n",
    "            \"Symbol\": symbol,\n",
    "            \"Price (Intraday)\": item[\"Price (Intraday)\"],\n",
    "            \"Change\": item[\"Change\"],\n",
    "            \"% Change\": item[\"% Change\"]\n",
    "        }\n",
    "        price_data.append(price_item)\n",
    "\n",
    "        volume_item = {\n",
    "            \"Symbol\": symbol,\n",
    "            \"Total Volume All Currencies in 24hr (in M)\": item[\"Total Volume All Currencies (24Hr)\"]\n",
    "        }\n",
    "        volume_data.append(volume_item)\n",
    "\n",
    "    # Save the data to separate JSON files in the data folder\n",
    "    with open('../data/cryptocurrency_data.json', \"w\") as file:\n",
    "        json.dump(cryptocurrency_data, file, indent=4)\n",
    "\n",
    "    with open('../data/price_data.json', \"w\") as file:\n",
    "        json.dump(price_data, file, indent=4)\n",
    "\n",
    "    with open('../data/volume_data.json', \"w\") as file:\n",
    "        json.dump(volume_data, file, indent=4)\n",
    "\n",
    "    print(\"Data scraped to JSON files successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating driver\n",
      "Web Title:  Crypto Real Time Prices & Latest News - Yahoo Finance\n",
      "The web scraped at: 2023-07-16 00:26:13.456055\n",
      "Found 25 rows on Page : 1\n",
      "Parsing Page : 1\n",
      "Total rows scraped : 25\n",
      "== Moving to Next Page ==\n",
      "Found 25 rows on Page : 2\n",
      "Parsing Page : 2\n",
      "Total rows scraped : 50\n",
      "== Moving to Next Page ==\n",
      "Found 25 rows on Page : 3\n",
      "Parsing Page : 3\n",
      "Total rows scraped : 75\n",
      "== Moving to Next Page ==\n",
      "Found 25 rows on Page : 4\n",
      "Parsing Page : 4\n",
      "Total rows scraped : 100\n",
      "Done Parsing...\n",
      "Data scraped to JSON files successfully!\n"
     ]
    }
   ],
   "source": [
    "YAHOO_FINANCE_URL = 'https://finance.yahoo.com/crypto/'\n",
    "TOTAL_CRYPTO = 100\n",
    "scrape_data(YAHOO_FINANCE_URL, TOTAL_CRYPTO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
