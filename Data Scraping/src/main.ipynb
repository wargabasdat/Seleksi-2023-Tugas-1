{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Watch Product in *jamtangan.com* with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchWindowException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import traceback\n",
    "\n",
    "from email.message import EmailMessage\n",
    "import ssl, smtplib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_isoformat(input):\n",
    "    month_dict = {\n",
    "        \"Jan\": \"01\",\n",
    "        \"Feb\": \"02\",\n",
    "        \"Mar\": \"03\",\n",
    "        \"Apr\": \"04\",\n",
    "        \"Mei\": \"05\",\n",
    "        \"Jun\": \"06\",\n",
    "        \"Jul\": \"07\",\n",
    "        \"Ags\": \"08\",\n",
    "        \"Sep\": \"09\",\n",
    "        \"Okt\": \"10\",\n",
    "        \"Nov\": \"11\",\n",
    "        \"Des\": \"12\"\n",
    "    }\n",
    "\n",
    "    for key, value in month_dict.items():\n",
    "        input = input.replace(key, value)\n",
    "    \n",
    "    datetime_object = datetime.strptime(input, \"%d %m %Y, %H:%M WIB\")\n",
    "\n",
    "    date = datetime_object.date()\n",
    "    time = datetime_object.time()\n",
    "\n",
    "    return [date, time]\n",
    "\n",
    "def to_email(name):\n",
    "    name = name.lower().strip()\n",
    "    email = name.replace(\" \", \"\") + \"@gmail.com\"\n",
    "    return email\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    json_array = []\n",
    "\n",
    "    with open(csv_file_path, encoding='utf-8') as csvf: \n",
    "        csv_reader = csv.DictReader(csvf) \n",
    "\n",
    "        for row in csv_reader: \n",
    "            json_array.append(row)\n",
    "\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as jsonf: \n",
    "        json_string = json.dumps(json_array, indent=4)\n",
    "        jsonf.write(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Product Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product(driver, wait):\n",
    "    product_dict = {\n",
    "        \"Brand\" : \"\",\n",
    "        \"Model No\" : \"\",\n",
    "        \"Series\" : \"\",\n",
    "        \"Gender\" : \"\",\n",
    "        \"Colour\" : \"\",\n",
    "        \"Luminous\" : \"\",\n",
    "        \"Calendar\" : \"\",\n",
    "        \"Water Resistant\" : \"\",\n",
    "        \"Case Diameter (mm)\" : \"\",\n",
    "        \"Strap Material\" : \"\",\n",
    "    }\n",
    "\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".tab-content .grid\")))\n",
    "    spec_grid = driver.find_element(By.CSS_SELECTOR, \".tab-content .grid\")\n",
    "    spec_list = spec_grid.find_elements(By.CSS_SELECTOR, \".spec-item\")\n",
    "    \n",
    "    for spec in spec_list:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".leading-6\")))\n",
    "        spec_title = spec.find_element(By.CSS_SELECTOR, \".font-black\").text\n",
    "        data = spec.find_elements(By.CSS_SELECTOR, \".leading-6\")\n",
    "        if spec_title in product_dict.keys():\n",
    "            key = data[0].text.strip()\n",
    "            value = data[1].text.strip()\n",
    "            product_dict[key] = value\n",
    "        elif (spec_title == \"Case Diameter\"):\n",
    "            value = float(data[1].text.replace(\"mm\", \"\").strip())\n",
    "            product_dict[\"Case Diameter (mm)\"] = value\n",
    "    \n",
    "    for (key, value) in product_dict.items():\n",
    "        if value == \"\": product_dict[key] = None\n",
    "\n",
    "    return product_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Sales Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sales(driver, wait):\n",
    "    sales_dict = {\n",
    "        \"Product Name\" : \"\",\n",
    "        \"Brand\" : \"\",\n",
    "        \"Model No\" : \"\",\n",
    "        \"Normal Price\" : \"\",\n",
    "        \"Discounted Price\" : \"\",\n",
    "        \"Discount Percentage\" : \"\",\n",
    "        \"Number of Seen\" : \"\",\n",
    "        \"Number of Sold\" : \"\",\n",
    "        \"Offline Stock Status\" : \"\",\n",
    "        \"Online Stock Status\" : \"\",\n",
    "    }\n",
    "\n",
    "    spec_grid = driver.find_element(By.CSS_SELECTOR, \".tab-content .grid\")\n",
    "    spec_list_raw = spec_grid.find_elements(By.CSS_SELECTOR, \".spec-item\")\n",
    "    for spec in spec_list_raw:\n",
    "        if spec.find_element(By.CSS_SELECTOR, \".font-black\").text in [\"Brand\", \"Model No\"]:\n",
    "            data = spec.find_elements(By.CSS_SELECTOR, \".leading-6\")\n",
    "            key = data[0].text.strip()\n",
    "            value = data[1].text.strip()\n",
    "            sales_dict[key] = value\n",
    "\n",
    "    sales_dict[\"Product Name\"] = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "\n",
    "    try:\n",
    "        normal_price = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='test-product-info'] .line-through\").text.strip()\n",
    "    except:\n",
    "        normal_price = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='test-product-info'] .text-xl\").text.strip()\n",
    "    discounted_price = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='test-product-info'] .text-xl\").text.strip()\n",
    "    sales_dict[\"Normal Price\"] = int(normal_price.replace(\"Rp\", \"\").replace(\".\", \"\").strip())\n",
    "    sales_dict[\"Discounted Price\"] = int(discounted_price.replace(\"Rp\", \"\").replace(\".\", \"\").strip())\n",
    "\n",
    "    discount_percentage = (sales_dict[\"Normal Price\"] - sales_dict[\"Discounted Price\"]) / sales_dict[\"Normal Price\"]\n",
    "    sales_dict[\"Discount Percentage\"] = round(discount_percentage * 100, 2)\n",
    "\n",
    "    num_seen = driver.find_element(By.CSS_SELECTOR, \".ic-eye + div > .text-sm\").text.strip()\n",
    "    if (num_seen.__contains__(\"Rb\")):\n",
    "        num_seen = float(num_seen.replace(\" Rb\", \"\").strip()) * 1000\n",
    "    sales_dict[\"Number of Seen\"] = int(num_seen)\n",
    "\n",
    "    num_sold = driver.find_element(By.CSS_SELECTOR, \".ic-cart.mr-1 + div > .text-sm\").text.strip()\n",
    "    if (num_sold.__contains__(\"Rb\")):\n",
    "        num_sold = float(num_sold.replace(\" Rb\", \"\").strip()) * 1000\n",
    "    sales_dict[\"Number of Sold\"] = int(num_sold)\n",
    "\n",
    "    try:\n",
    "        empty_badge = driver.find_element(By.CSS_SELECTOR, \".badge.bg-accent-red\")\n",
    "        if (empty_badge != None and empty_badge.text.strip().__contains__(\"habis\")):\n",
    "            sales_dict[\"Online Stock Status\"] = \"Not Available\"\n",
    "        else:\n",
    "            raise Exception(\"The badge is not empty badge\")\n",
    "    except:\n",
    "        online_stock_status = driver.find_element(By.CSS_SELECTOR, \".stepper-wrapper + div\").text.strip()\n",
    "        if (online_stock_status == \"STOK ONLINE < 5 PCS\"): sales_dict[\"Online Stock Status\"] = \"Low (< 5 PCS)\"\n",
    "        elif (online_stock_status == \"STOK ONLINE > 5 PCS\"): sales_dict[\"Online Stock Status\"] = \"High (>= 5 PCS)\"\n",
    "        else: sales_dict[\"Online Stock Status\"] = \"Unknown\"\n",
    "\n",
    "    try:\n",
    "        offline_empty = driver.find_element(By.CSS_SELECTOR, \"picture.mr-2 + div\")\n",
    "        if (offline_empty != None and offline_empty.text.strip().__contains__(\"Tidak tersedia\")):\n",
    "            sales_dict[\"Offline Stock Status\"] = \"Not Available\"\n",
    "        else:\n",
    "            raise Exception(\"The text is not empty text\")\n",
    "    except:\n",
    "        try:\n",
    "            offline_stock_status = driver.find_element(By.CSS_SELECTOR, \"div[data-testid='store-item-0']\")\n",
    "            if (offline_stock_status != None):\n",
    "                sales_dict[\"Offline Stock Status\"] = \"Available\"\n",
    "        except:\n",
    "            sales_dict[\"Offline Stock Status\"] = \"Unknown\"\n",
    "\n",
    "    return sales_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Customer and Review Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_customer_review(driver, wait):\n",
    "    customer_dict = {\n",
    "        \"Email\" : \"\",\n",
    "        \"Name\" : \"\",\n",
    "        \"Member Status\" : \"\"\n",
    "    }\n",
    "    customer_header = customer_dict.keys()\n",
    "    customer_list = []\n",
    "\n",
    "    review_dict = {\n",
    "        \"Product Name\" : \"\",\n",
    "        \"Email\" : \"\",\n",
    "        \"Date\" : \"\",\n",
    "        \"Time\" : \"\",\n",
    "        \"Rating\" : \"\",\n",
    "        \"Delivery Review\" : \"\",\n",
    "        \"Product Review\" : \"\"\n",
    "    }\n",
    "    review_header = review_dict.keys()\n",
    "    review_list = []\n",
    "\n",
    "    MAX_REVIEW_PER_PRODUCT = 5\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".pb-14\")))\n",
    "    div_review_pagination = driver.find_element(By.CSS_SELECTOR, \".pb-14\")\n",
    "    try:\n",
    "        max_tab_per_product = int(div_review_pagination.find_elements(By.TAG_NAME, \"li\")[-2].text)\n",
    "    except:\n",
    "        max_tab_per_product = 1\n",
    "\n",
    "    review_count = 0\n",
    "    tab_count = 0\n",
    "    # driver.implicitly_wait(10)\n",
    "    while (review_count < MAX_REVIEW_PER_PRODUCT and tab_count < max_tab_per_product):\n",
    "        time.sleep(1)\n",
    "        rating_div = driver.find_elements(By.CSS_SELECTOR, \".bg-neutral-900.col-span-12\")[1]\n",
    "        review_divs = rating_div.find_elements(By.CSS_SELECTOR, \".p-4.relative\")\n",
    "\n",
    "        for review in review_divs:\n",
    "            reviewer_name = review.find_element(By.CSS_SELECTOR, \".mb-1 .text-base\").text\n",
    "            if (len(reviewer_name) < 3) or (reviewer_name[1] == '*' and reviewer_name[-2] == '*'): continue\n",
    "\n",
    "            product_name = driver.find_element(By.CSS_SELECTOR, \"h1\").text\n",
    "\n",
    "            rating = len(review.find_elements(By.CSS_SELECTOR, \".rating .ic-star-fill\"))\n",
    "            datetime_review = to_isoformat(review.find_element(By.CSS_SELECTOR, \"span.block.text-xxs\").text)\n",
    "            date_review, time_review = datetime_review[0], datetime_review[1]\n",
    "            \n",
    "            paragraph_review = review.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "            delivery_review = \"\"\n",
    "            product_review = \"\"\n",
    "            for par in paragraph_review:\n",
    "                title = par.find_element(By.CSS_SELECTOR, \"span\").text\n",
    "                if title == \"Pengiriman:\": delivery_review = par.text.lstrip(\"Pengiriman: \")\n",
    "                if title == \"Produk:\": product_review = par.text.lstrip(\"Produk: \")\n",
    "                else: continue\n",
    "            if delivery_review == \"\": delivery_review = \"Tidak ada review\"\n",
    "            if product_review == \"\": product_review = \"Tidak ada review\"\n",
    "\n",
    "            member_status = driver.find_element(By.CSS_SELECTOR, \".badge.text-xxs\").text\n",
    "\n",
    "            review_count += 1\n",
    "            if review_count > MAX_REVIEW_PER_PRODUCT: break\n",
    "\n",
    "            customer_result = [to_email(reviewer_name), reviewer_name, member_status]\n",
    "            for key, value in zip(customer_header, customer_result):\n",
    "                customer_dict[key] = value\n",
    "            customer_list.append(customer_dict.copy())\n",
    "        \n",
    "            result_review = [product_name, to_email(reviewer_name), date_review, time_review, rating, delivery_review, product_review]\n",
    "            for key, value in zip(review_header, result_review):\n",
    "                review_dict[key] = value\n",
    "            review_list.append(review_dict.copy())\n",
    "        \n",
    "        tab_count += 1\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".pb-14\")))\n",
    "        div_review_pagination = driver.find_element(By.CSS_SELECTOR, \".pb-14\")\n",
    "        page_buttons = div_review_pagination.find_elements(By.TAG_NAME, \"li\")\n",
    "        for page_button in page_buttons:\n",
    "            if page_button.text == str(tab_count + 1): \n",
    "                page_button.click()\n",
    "                break\n",
    "\n",
    "    return [customer_list, review_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # instantiate the driver and the wait object\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # define the URL\n",
    "    URL = \"https://www.jamtangan.com/c/jam-tangan\"\n",
    "\n",
    "    # open the URL\n",
    "    driver.maximize_window()\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        # close the pop-up if it exists\n",
    "        try:\n",
    "            if (driver.find_element(By.ID, \"driver-popover-item\")):\n",
    "                driver.find_element(By.CLASS_NAME, \"driver-close-btn\").click()\n",
    "\n",
    "            time.sleep(3)\n",
    "            if (driver.find_element(By.CSS_SELECTOR, \"button.ng-binding\")):\n",
    "                driver.find_element(By.CSS_SELECTOR, \"button.ng-binding\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # expand the brands list in filter sidebar\n",
    "        expand_brand = driver.find_elements(By.CSS_SELECTOR, \".accordion-content-wrapper .cursor-pointer\")[1]\n",
    "        while(expand_brand.text == \"LIHAT LAINNYA\"):\n",
    "            expand_brand.click()\n",
    "            time.sleep(1)\n",
    "            expand_brand = driver.find_elements(By.CSS_SELECTOR, \".accordion-content-wrapper .cursor-pointer\")[1]\n",
    "\n",
    "        # get the brands name and url that eligible to be scraped\n",
    "        brands_raw = driver.find_elements(By.CSS_SELECTOR, \".accordion-content-wrapper\")[1].find_elements(By.TAG_NAME, \"li\")\n",
    "        eliminate = ['Semua Brand', 'Band', 'Strap', 'Bracelet', 'Accessories', 'Jewelry', 'Wallets']\n",
    "        brands = []\n",
    "        for brand in brands_raw:\n",
    "            eliminated = False\n",
    "            for phrase in eliminate:\n",
    "                if (phrase in brand.find_element(By.TAG_NAME, \"label\").text):\n",
    "                    eliminated = True\n",
    "                    break\n",
    "            if not eliminated:\n",
    "                brands.append(brand)\n",
    "        brands = brands[39:]\n",
    "        brands_name = [brand.find_element(By.TAG_NAME, \"label\").text for brand in brands]\n",
    "        brands_url = [brand.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") for brand in brands]\n",
    "\n",
    "        # declare the container of the tables\n",
    "        product = []\n",
    "        sales = []\n",
    "        customer = []\n",
    "        review = []\n",
    "\n",
    "        # scrape the data by iterating each product through the brands\n",
    "        max_product_per_brand = 20\n",
    "        product_count = 1\n",
    "        for brand_url in brands_url:\n",
    "            driver.get(brand_url)\n",
    "            wait.until(EC.url_to_be(brand_url))\n",
    "            time.sleep(1)\n",
    "            products_tag = driver.find_elements(By.CSS_SELECTOR, \"a[data-testid='product-card-test']\")[:max_product_per_brand]\n",
    "            products_url = [product.get_attribute(\"href\") for product in products_tag]\n",
    "            for product_url in products_url:\n",
    "                try:\n",
    "                    driver.get(product_url)\n",
    "                    wait.until(EC.url_to_be(product_url))\n",
    "                    print(f\"{product_count}. Brand = {brands_name[brands_url.index(brand_url)]} | Product = {product_url}\")\n",
    "                    extracted_product = extract_product(driver, wait)\n",
    "                    extracted_sales = extract_sales(driver, wait)\n",
    "                    extracted_customer = extract_customer_review(driver, wait)[0]\n",
    "                    extracted_review = extract_customer_review(driver, wait)[1]\n",
    "\n",
    "                    product.append(extracted_product)\n",
    "                    sales.append(extracted_sales)\n",
    "                    for item in extracted_customer: \n",
    "                        if (item not in customer):\n",
    "                            customer.append(item)\n",
    "                    for item in extracted_review: review.append(item)\n",
    "                    product_count += 1\n",
    "                except:\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "        driver.close()  \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        pass        \n",
    "\n",
    "    try:\n",
    "        save_dir = r'../data/unprocessed/'\n",
    "\n",
    "        # exporting the product table to csv and json\n",
    "        with open(save_dir + \"product.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=product[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(product)\n",
    "        product_csv_file_path = save_dir + r'product.csv'\n",
    "        product_json_file_path = save_dir + r'product.json'\n",
    "        csv_to_json(product_csv_file_path, product_json_file_path)\n",
    "\n",
    "        # exporting the sales table to csv and json\n",
    "        with open(save_dir + \"sales.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=sales[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(sales)\n",
    "        sales_csv_file_path = save_dir + r'sales.csv'\n",
    "        sales_json_file_path = save_dir + r'sales.json'\n",
    "        csv_to_json(sales_csv_file_path, sales_json_file_path)\n",
    "\n",
    "        # exporting the customer table to csv and json\n",
    "        with open(save_dir + \"customer.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=customer[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(customer)\n",
    "        customer_csv_file_path = save_dir + r'customer.csv'\n",
    "        customer_json_file_path = save_dir + r'customer.json'\n",
    "        csv_to_json(customer_csv_file_path, customer_json_file_path)\n",
    "\n",
    "        # exporting the review table to csv and json\n",
    "        with open(save_dir + \"review.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=review[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(review)\n",
    "        review_csv_file_path = save_dir + r'review.csv'\n",
    "        review_json_file_path = save_dir + r'review.json'\n",
    "        csv_to_json(review_csv_file_path, review_json_file_path)\n",
    "\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "    \n",
    "    print(f\"Product Collected = {len(product)}\")\n",
    "    print(f\"Sales Collected = {len(sales)}\")\n",
    "    print(f\"Customer Collected = {len(customer)}\")\n",
    "    print(f\"Review Collected = {len(review)}\")\n",
    "\n",
    "    print(\"\\n========== Scraping is done! ==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Collected = 692\n",
      "Sales Collected = 692\n",
      "Customer Collected = 1325\n",
      "Review Collected = 1422\n",
      "\n",
      "========== Scraping is done! ==========\n",
      "\n",
      "Time Start:\t2023-07-18 16:28:13\n",
      "Time End:\t2023-07-18 18:24:43\n",
      "Duration:\t1:56:30  \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if (__name__ == \"__main__\"):\n",
    "    # setup the start timer\n",
    "    time_start = datetime.now()\n",
    "\n",
    "    # run the main program\n",
    "    main()\n",
    "\n",
    "    # stop the end timer\n",
    "    time_end = datetime.now()\n",
    "    delta = time_end - time_start\n",
    "\n",
    "    text_wrapper = open(r\"Data Scraping\\src\\config\\email_config.txt\", \"r\")\n",
    "\n",
    "    email_config = text_wrapper.readlines()\n",
    "    email_to_notify = email_config[0].strip()\n",
    "    email_password = email_config[1].strip()\n",
    "\n",
    "    text_wrapper.close()\n",
    "\n",
    "    subject = \"Scraping Jamtangan.com Report\"\n",
    "    body = f\"\"\"\n",
    "    [SCRAPING JAMTANGAN.COM IS COMPLETE]\n",
    "\n",
    "    Time Start:\\t{time_start.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    Time End:\\t{time_end.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    Duration:\\t{str(delta).split('.')[0]}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    email = EmailMessage()\n",
    "    email[\"From\"] = email_to_notify\n",
    "    email[\"To\"] = email_to_notify\n",
    "    email[\"Subject\"] = subject\n",
    "    email.set_content(body)\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "        server.login(email_to_notify, email_password)\n",
    "        server.sendmail(email_to_notify, email_to_notify, email.as_string())\n",
    "    \n",
    "    print(f\"Time Start:\\t{time_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Time End:\\t{time_end.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration:\\t{str(delta).split('.')[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing Data Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Jumlah data product yang duplikat: 0\n",
      "===== Jumlah raw data product include bad product: 1339\n",
      "===== Contoh bad product: \n",
      "            Brand               Model No    Series Gender Colour Luminous  \\\n",
      "173  Boda Concept  BC-C1-CF-CARBON-FIBER  C Series                          \n",
      "402           DLW   HANDS-MIL-SPEC-BLACK                                    \n",
      "403           DLW    FLAT-BI-CI-SKX-BLUE                                    \n",
      "\n",
      "    Calendar Water Resistant Case Diameter (mm) Strap Material  \n",
      "173                                                             \n",
      "402                                                             \n",
      "403                                                             \n",
      "===== Jumlah raw data product setelah filtering bad product: 1252\n",
      "===== Jumlah data product yang memiliki nilai kosong: 647\n",
      "===== Jumlah data product yang memiliki nilai kosong setelah dihandle: 0\n"
     ]
    }
   ],
   "source": [
    "# load the product data\n",
    "unprocessed_product_df = pd.read_json(r\"../data/unprocessed/product.json\")\n",
    "\n",
    "# check duplicate data\n",
    "duplicated_product_stat = unprocessed_product_df.duplicated()\n",
    "num_duplicated_product = duplicated_product_stat.sum()\n",
    "\n",
    "print(f\"===== Jumlah data product yang duplikat: {num_duplicated_product}\")\n",
    "\n",
    "# check and handle bad product\n",
    "print(f\"===== Jumlah raw data product include bad product: {len(unprocessed_product_df)}\")\n",
    "missing_amount = unprocessed_product_df.eq(\"\").sum(axis=1)\n",
    "filtered_product_df = unprocessed_product_df[missing_amount <= 2]\n",
    "print(f\"===== Contoh bad product: \\n{unprocessed_product_df[missing_amount > 2].head(3)}\")\n",
    "print(f\"===== Jumlah raw data product setelah filtering bad product: {len(filtered_product_df)}\")\n",
    "\n",
    "# check and handle missing value\n",
    "missing_value = (filtered_product_df == \"\").sum().sum()\n",
    "print(f\"===== Jumlah data product yang memiliki nilai kosong: {missing_value}\")\n",
    "\n",
    "filtered_product_df = filtered_product_df.replace(\"\", \"-\")\n",
    "missing_value = (filtered_product_df == \"\").sum().sum()\n",
    "print(f\"===== Jumlah data product yang memiliki nilai kosong setelah dihandle: {missing_value}\")\n",
    "\n",
    "# export the filtered product data\n",
    "file_path = r\"../data/processed/product.json\"\n",
    "df_json = filtered_product_df.to_json(file_path, orient=\"records\")\n",
    "\n",
    "# export to csv\n",
    "file_path = r\"../data/processed/product.csv\"\n",
    "filtered_product_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing Data Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Jumlah data sales yang duplikat: 0\n",
      "===== Jumlah raw data sales include bad product: 1339\n",
      "===== Jumlah raw data sales setelah filtering bad product: 1252\n"
     ]
    }
   ],
   "source": [
    "# load the sales data\n",
    "unprocessed_sales_df = pd.read_json(r\"../data/unprocessed/sales.json\")\n",
    "\n",
    "# check duplicate data\n",
    "duplicated_sales_stat = unprocessed_sales_df.duplicated()\n",
    "num_duplicated_sales = duplicated_sales_stat.sum()\n",
    "\n",
    "print(f\"===== Jumlah data sales yang duplikat: {num_duplicated_sales}\")\n",
    "\n",
    "# remove sales that include bad product\n",
    "print(f\"===== Jumlah raw data sales include bad product: {len(unprocessed_sales_df)}\")\n",
    "filtered_sales_df = unprocessed_sales_df[\n",
    "    unprocessed_sales_df[\"Brand\"].isin(filtered_product_df[\"Brand\"]) & \n",
    "    unprocessed_sales_df[\"Model No\"].isin(filtered_product_df[\"Model No\"])]\n",
    "print(f\"===== Jumlah raw data sales setelah filtering bad product: {len(filtered_sales_df)}\")\n",
    "\n",
    "# export the filtered sales data\n",
    "file_path = r\"../data/processed/sales.json\"\n",
    "df_json = filtered_sales_df.to_json(file_path, orient=\"records\")\n",
    "\n",
    "# export to csv\n",
    "file_path = r\"../data/processed/sales.csv\"\n",
    "filtered_sales_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing Data Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Jumlah data customer yang duplikat (email yang sama): 229\n",
      "===== Contoh data dengan email yang sama dan member status yang bisa berbeda:\n",
      "                    Email        Name     Member Status\n",
      "1653  abdulazis@gmail.com  Abdul azis  Centurion Member\n",
      "1998  abdulazis@gmail.com  Abdul azis      Basic Member\n",
      "764         ade@gmail.com         Ade     Silver Member\n",
      "2195        ade@gmail.com         Ade      Basic Member\n",
      "1012      agung@gmail.com       agung      Basic Member\n",
      "206       agung@gmail.com       Agung      Basic Member\n",
      "\n",
      "===== Jumlah data customer yang duplikat setelah diproses: 0\n"
     ]
    }
   ],
   "source": [
    "# load the customer data\n",
    "unprocessed_customer_df = pd.read_json(r\"../data/unprocessed/customer.json\")\n",
    "\n",
    "# check and handle duplicate data\n",
    "duplicated_cust_stat = unprocessed_customer_df.duplicated(subset=['Email'])\n",
    "num_duplicated_cust = duplicated_cust_stat.sum()\n",
    "print(f\"===== Jumlah data customer yang duplikat (email yang sama): {num_duplicated_cust}\")\n",
    "\n",
    "print(\"===== Contoh data dengan email yang sama dan member status yang bisa berbeda:\")\n",
    "df_cust_email_sorted = unprocessed_customer_df.sort_values(by='Email')\n",
    "duplicated_cust_stat = df_cust_email_sorted.duplicated(subset='Email', keep=False)\n",
    "duplicated_cust_rows = df_cust_email_sorted[duplicated_cust_stat]\n",
    "print(duplicated_cust_rows[:6])\n",
    "\n",
    "# delete duplicated rows with same email and keep the highest member status\n",
    "duplicateless_cust = df_cust_email_sorted.drop_duplicates(subset='Email', keep='first')\n",
    "\n",
    "print(f\"\\n===== Jumlah data customer yang duplikat setelah diproses: {duplicateless_cust.duplicated(subset=['Email']).sum()}\")\n",
    "\n",
    "# export the filtered customer data\n",
    "file_path = r\"../data/processed/customer.json\"\n",
    "df_json = duplicateless_cust.to_json(file_path, orient=\"records\")\n",
    "\n",
    "# export to csv\n",
    "file_path = r\"../data/processed/customer.csv\"\n",
    "duplicateless_cust.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Jumlah raw data review include bad product: 2650\n",
      "===== Jumlah raw data review setelah filtering bad product: 1814\n",
      "===== Jumlah data review yang duplikat: 268\n",
      "===== Data yang duplikat:\n",
      "                                           Product Name  \\\n",
      "57    Alba Prestige AJ6124X1 Men Silver Dial Dual To...   \n",
      "131   Alexandre Christie Chronograph AC 6650 MC REPB...   \n",
      "171   Alexandre Christie Primo Steel AC 1017 MDBBRBA...   \n",
      "195   Armani Exchange AX2101 Whitman Black Dial Blac...   \n",
      "196   Armani Exchange AX2101 Whitman Black Dial Blac...   \n",
      "...                                                 ...   \n",
      "2589  TISSOT T-Sport T116.617.11.047.01 Chrono XL Cl...   \n",
      "2590  TISSOT T-Sport T116.617.11.047.01 Chrono XL Cl...   \n",
      "2592  TISSOT Luxury Powermatic 80 T086.407.11.201.02...   \n",
      "2594  TISSOT T-Sport T116.617.11.057.01 Chrono XL Cl...   \n",
      "2596  Tissot PR 100 Chronograph Gent White Dial Stai...   \n",
      "\n",
      "                         Email        Date      Time  Rating  \\\n",
      "57             melda@gmail.com  2022-03-30  11:09:00       5   \n",
      "131         yusrizal@gmail.com  2023-04-17  20:34:00       4   \n",
      "171            tommy@gmail.com  2022-08-05  12:40:00       5   \n",
      "195   gloriastefanie@gmail.com  2019-09-16  12:23:00       5   \n",
      "196       yunaidyjie@gmail.com  2019-05-24  09:55:00       5   \n",
      "...                        ...         ...       ...     ...   \n",
      "2589   suhardifonger@gmail.com  2019-03-23  11:15:00       5   \n",
      "2590            deny@gmail.com  2019-02-18  14:37:00       5   \n",
      "2592  wayansudiarsha@gmail.com  2023-04-06  15:56:00       5   \n",
      "2594  mindoedisinaga@gmail.com  0001-01-01  00:00:00       5   \n",
      "2596          cening@gmail.com  2018-02-02  18:20:00       5   \n",
      "\n",
      "       Delivery Review                                     Product Review  \n",
      "57    Tidak ada review                                    Alba sllu keren  \n",
      "131   Tidak ada review                                   Tidak ada review  \n",
      "171   Tidak ada review                                   Tidak ada review  \n",
      "195   Tidak ada review                       Barang oke sesuai lah mantap  \n",
      "196   Tidak ada review  as banget dgn jam ini... murah tapi berkelas, ...  \n",
      "...                ...                                                ...  \n",
      "2589  Tidak ada review             iginal, pengiriman cepat, overall puas  \n",
      "2590  Tidak ada review  Yes, finally I found this product at www.jamta...  \n",
      "2592  Tidak ada review  Kualitas barang sangat bagus, Packing sangat a...  \n",
      "2594  Tidak ada review  Tissot chrono XL menampilkan bentuk yg menarik...  \n",
      "2596  Tidak ada review       Kalau beli langsung ke toko alamatnya dimana  \n",
      "\n",
      "[268 rows x 7 columns]\n",
      "===== Jumlah data review yang duplikat setelah diproses: 0\n"
     ]
    }
   ],
   "source": [
    "# load the review data\n",
    "unprocessed_review_df = pd.read_json(r\"../data/unprocessed/review.json\")\n",
    "\n",
    "# remove review that include bad product\n",
    "print(f\"===== Jumlah raw data review include bad product: {len(unprocessed_review_df)}\")\n",
    "filtered_review_df = unprocessed_review_df[\n",
    "    unprocessed_review_df['Product Name'].isin(filtered_sales_df['Product Name']) & \n",
    "    unprocessed_review_df['Email'].isin(duplicateless_cust['Email'])]\n",
    "print(f\"===== Jumlah raw data review setelah filtering bad product: {len(filtered_review_df)}\")\n",
    "\n",
    "# check and handle duplicate data\n",
    "duplicated_review_stat = filtered_review_df.duplicated()\n",
    "num_duplicated_review = duplicated_review_stat.sum()\n",
    "duplicated_review_rows = filtered_review_df[duplicated_review_stat]\n",
    "duplicateless_review = filtered_review_df.drop_duplicates()\n",
    "\n",
    "print(f\"===== Jumlah data review yang duplikat: {num_duplicated_review}\")\n",
    "print(f\"===== Data yang duplikat:\\n{duplicated_review_rows}\")\n",
    "print(f\"===== Jumlah data review yang duplikat setelah diproses: {duplicateless_review.duplicated().sum()}\")\n",
    "\n",
    "# export the filtered review data\n",
    "file_path = r\"../data/processed/review.json\"\n",
    "df_json = duplicateless_review.to_json(file_path, orient=\"records\")\n",
    "\n",
    "# export to csv\n",
    "file_path = r\"../data/processed/review.csv\"\n",
    "duplicateless_review.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
