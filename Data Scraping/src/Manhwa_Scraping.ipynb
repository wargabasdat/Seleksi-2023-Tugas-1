{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 1000 Manhwa Web Scraper\n",
    "### Tugas Seleksi 1 Calon Asisten Lab Basis Data\n",
    "### Wan Aufa Azis - 18221001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function and Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get manhwa_data from each url\n",
    "def get_manhwa_data(url, headers):\n",
    "    manhwa_webpage = requests.get(url, headers=headers)\n",
    "\n",
    "    new_soup = BeautifulSoup(manhwa_webpage.content, \"html.parser\")\n",
    "\n",
    "    # Get soup left_side of page \n",
    "    left_side = new_soup.find('div', attrs={'class' : 'leftside'})\n",
    "    attrs = left_side.find_all('div', {'class' : 'spaceit_pad'})\n",
    "\n",
    "    # Get att name\n",
    "    name = new_soup.find('span', attrs={'class' : 'h1-title'}).text\n",
    "    \n",
    "    # Store genre and author list\n",
    "    gen_list = []\n",
    "    auth_list = []\n",
    "\n",
    "    # Loop for extract manhwa attributes from left_side page soup\n",
    "    for att in attrs:\n",
    "        if(\"Volumes\" in att.text):\n",
    "            vol = att.text.strip('Volumes:').strip()\n",
    "            if(vol=='Unknown'):\n",
    "                vol = None\n",
    "            else:\n",
    "                vol = int(vol)\n",
    "        elif(\"Chapters\" in att.text):\n",
    "            chap = att.text.strip('Chapters:').strip()\n",
    "            if(chap=='Unknown'):\n",
    "                chap = None\n",
    "            else:\n",
    "                chap = int(chap)\n",
    "        elif(\"Status\" in att.text):\n",
    "            stat = att.text.strip('Status:').strip()\n",
    "        elif(\"Published\" in att.text):\n",
    "            try:\n",
    "                pubs = att.text.strip('Published:').strip().split(' to ')\n",
    "                pubs_starts = pubs[0]\n",
    "                pubs_ends = pubs[1]\n",
    "                if(pubs_ends=='?'):\n",
    "                    pubs_ends = None\n",
    "                else:\n",
    "                    pubs_ends = datetime.strptime(pubs_ends, \"%b %d, %Y\")\n",
    "                pubs_starts = datetime.strptime(pubs_starts, \"%b %d, %Y\")\n",
    "            except:\n",
    "                pubs_starts = None\n",
    "                pubs_ends = None\n",
    "        elif(\"Genre\" in att.text):\n",
    "            gens = att.find_all('span', {'itemprop' : 'genre'})\n",
    "            for gen in gens:\n",
    "                gen_list.append(gen.text)\n",
    "        elif(\"Serialization\" in att.text):\n",
    "            serial = att.text.strip('Serialization:').strip()\n",
    "            if(serial=='N'):serial=None\n",
    "        elif(\"Authors\" in att.text):\n",
    "            authors = att.find_all('a')\n",
    "            for auth in authors:\n",
    "                auth_list.append(auth.text)\n",
    "        elif(\"Score\" in att.text):\n",
    "            score = float(att.find('span', {'itemprop' : 'ratingValue'}).text.strip())\n",
    "        elif(\"Ranked\" in att.text):\n",
    "            rank = att.find('span').next.next.text.strip()\n",
    "            rank = int(rank[1:])\n",
    "        elif(\"Popularity\" in att.text):\n",
    "            pop_rank = int(att.text.strip('Popularity: #'))\n",
    "        elif(\"Members\" in att.text):\n",
    "            memb = att.text.strip('Members:').strip()\n",
    "            if(',' in memb):\n",
    "                memb = memb.replace(',','')\n",
    "            memb = int(memb)\n",
    "        elif(\"Favorites\" in att.text):\n",
    "            fav = att.text.strip('Favorites:').strip()\n",
    "            if(',' in fav):\n",
    "                fav = fav.replace(',','')\n",
    "            fav = int(fav)\n",
    "        \n",
    "    return name, vol, chap, stat, pubs_starts, pubs_ends, gen_list, serial, auth_list, score, rank, pop_rank, memb, fav\n",
    "\n",
    "# Function to write data to json file\n",
    "def write_json(new_data, filename='..\\data\\data_manhwa.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data.append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 5, default=str)\n",
    "\n",
    "# Function to add each data_lsit to arr\n",
    "def add_data_to_list(arr, data_list):\n",
    "    for data in data_list:\n",
    "        arr.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'English: Who Can Define Popularity?'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# Loop for extracting product details from each link\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links_list:\n\u001b[1;32m---> 49\u001b[0m     name, vol, chap, stat, pubs_starts, pubs_ends, gen_list, serial, auth_list, score, rank, pop_rank, memb, fav \u001b[39m=\u001b[39m get_manhwa_data(link, headers)\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: name,\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvolumes\u001b[39m\u001b[39m\"\u001b[39m: vol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfavorites\u001b[39m\u001b[39m\"\u001b[39m: fav\n\u001b[0;32m     66\u001b[0m     }\n\u001b[0;32m     68\u001b[0m     \u001b[39m# Write data to json file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[178], line 66\u001b[0m, in \u001b[0;36mget_manhwa_data\u001b[1;34m(url, headers)\u001b[0m\n\u001b[0;32m     64\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(rank[\u001b[39m1\u001b[39m:])\n\u001b[0;32m     65\u001b[0m \u001b[39melif\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPopularity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m att\u001b[39m.\u001b[39mtext):\n\u001b[1;32m---> 66\u001b[0m     pop_rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(att\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mstrip(\u001b[39m'\u001b[39;49m\u001b[39mPopularity: #\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     67\u001b[0m \u001b[39melif\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMembers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m att\u001b[39m.\u001b[39mtext):\n\u001b[0;32m     68\u001b[0m     memb \u001b[39m=\u001b[39m att\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39mMembers:\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstrip()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'English: Who Can Define Popularity?'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Headers for request\n",
    "    headers = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "    \n",
    "    # Webpage url\n",
    "    url = 'https://myanimelist.net/topmanga.php?type=manhwa&limit=0'\n",
    "\n",
    "    #Chrome webdriver initialization\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "\n",
    "    # List of all manhwa links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop 20 times to get 1000 record\n",
    "    for i in range(20):\n",
    "        # Sub-webpage html\n",
    "        html = browser.page_source\n",
    "\n",
    "        # Soup object\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Links extraction\n",
    "        rows = soup.find_all('a', attrs={'class' : 'hoverinfo_trigger fs14 fw-b'})\n",
    "\n",
    "        # Store the links to links_list\n",
    "        for row in rows:\n",
    "            links_list.append(row.get('href'))\n",
    "\n",
    "        # Search for next sub-webpage button\n",
    "        next_button = browser.find_element(By.XPATH, '//a[@class=\"link-blue-box next\"]')\n",
    "\n",
    "        if next_button.is_enabled:\n",
    "            next_button.click()\n",
    "            time.sleep(10)\n",
    "\n",
    "    with open(\"..\\data\\data_manhwa.json\", \"w\") as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "    # Store all genre and author list\n",
    "    all_genre_list = []\n",
    "    all_author_list = []\n",
    "\n",
    "    # Loop for extracting product details from each link\n",
    "    for link in links_list:\n",
    "        name, vol, chap, stat, pubs_starts, pubs_ends, gen_list, serial, auth_list, score, rank, pop_rank, memb, fav = get_manhwa_data(link, headers)\n",
    "\n",
    "        data = {\n",
    "            \"name\": name,\n",
    "            \"volumes\": vol,\n",
    "            \"chapters\": chap,\n",
    "            \"status\": stat,\n",
    "            \"published_start\": pubs_starts,\n",
    "            \"published_end\": pubs_ends,\n",
    "            \"genres\": gen_list,\n",
    "            \"serialization\": serial,\n",
    "            \"authors\": auth_list,\n",
    "            \"score\": score,\n",
    "            \"ranked\": rank,\n",
    "            \"popularity_rank\": pop_rank,\n",
    "            \"members\": memb,\n",
    "            \"favorites\": fav\n",
    "        }\n",
    "\n",
    "        # Write data to json file\n",
    "        write_json(data)\n",
    "\n",
    "        # Add genre and author data to list\n",
    "        add_data_to_list(all_genre_list, gen_list)\n",
    "        add_data_to_list(all_author_list, auth_list)\n",
    "\n",
    "    # Remove duplicates value from list\n",
    "    all_genre_list = list(dict.fromkeys(all_genre_list))\n",
    "    all_author_list = list(dict.fromkeys(all_author_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Converting to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json('..\\data\\data_manhwa.json')\n",
    "\n",
    "# 1. Manhwa_Info Entity\n",
    "manhwa_info_df = df[['name', 'volumes', 'chapters', 'status', 'published_start', 'published_end', 'serialization']]\n",
    "manhwa_info_df.insert(0,'idmanhwa', range(1, len(manhwa_info_df)+1))\n",
    "\n",
    "# 2. Genre Entity\n",
    "genre_df = pd.DataFrame(all_genre_list, columns=['name'])\n",
    "genre_df.insert(0, 'idgenre', range(1, len(genre_df)+1))\n",
    "genre_df.insert(2, 'description', value=None)\n",
    "\n",
    "genre_df.to_csv('..\\data\\genre.csv', index=False)\n",
    "\n",
    "# 3. Manhwa_Genre Entity\n",
    "manhwa_genre_df = (df[['name', 'genres']]).explode('genres')\n",
    "\n",
    "manhwa_genre_df = manhwa_genre_df.merge(manhwa_info_df, left_on='name', right_on='name', how='left')\n",
    "manhwa_genre_df = manhwa_genre_df.drop(['name', 'volumes', 'chapters', 'status', 'published_start', 'published_end', 'serialization'], axis=1)\n",
    "\n",
    "manhwa_genre_df = manhwa_genre_df.merge(genre_df, left_on='genres', right_on='name', how='left')\n",
    "manhwa_genre_df = manhwa_genre_df.drop(['name', 'genres', 'description'], axis=1)\n",
    "\n",
    "manhwa_genre_df.to_csv('..\\data\\manhwa_genre.csv', index=False)\n",
    "\n",
    "# 4. Manhwa_Statistic Entity\n",
    "manhwa_statistic_df = df[['score', 'ranked', 'popularity_rank', 'members', 'favorites']]\n",
    "manhwa_statistic_df.insert(0,'manhwa', range(1, len(manhwa_statistic_df)+1))\n",
    "\n",
    "manhwa_statistic_df.to_csv('..\\data\\manhwa_statistic.csv', index=False)\n",
    "\n",
    "# 5. Author Entity\n",
    "author_df = pd.DataFrame(all_author_list, columns=['name'])\n",
    "author_df.insert(0, 'idauthor', range(1, len(author_df)+1))\n",
    "author_df.insert(2, 'family_name', value=None)\n",
    "author_df.insert(3, 'gender', value=None)\n",
    "author_df.insert(4, 'birthday', value=None)\n",
    "author_df.insert(5, 'website', value=None)\n",
    "author_df.insert(6, 'member_favorites', value=None)\n",
    "\n",
    "author_df.to_csv('../data/author.csv', index=False)\n",
    "\n",
    "# 6. Manhwa_Author Entity\n",
    "manhwa_author_df = (df[['name', 'authors']]).explode('authors')\n",
    "\n",
    "manhwa_author_df = manhwa_author_df.merge(manhwa_info_df, left_on='name', right_on='name', how='left')\n",
    "manhwa_author_df = manhwa_author_df.drop(['name', 'volumes', 'chapters', 'status', 'published_start', 'published_end', 'serialization'], axis=1)\n",
    "\n",
    "manhwa_author_df = manhwa_author_df.merge(author_df, left_on='authors', right_on='name', how='left')\n",
    "manhwa_author_df = manhwa_author_df.drop(['authors', 'name', 'family_name', 'gender', 'birthday', 'website', 'member_favorites'], axis=1)\n",
    "\n",
    "manhwa_author_df.to_csv('..\\data\\manhwa_author.csv', index=False)\n",
    "\n",
    "# 7. serialization entity\n",
    "serialization_df = df[['serialization']].dropna().drop_duplicates()\n",
    "serialization_df.insert(0, 'idserialization', range(1, len(serialization_df)+1))\n",
    "serialization_df.insert(2, 'website', value=None)\n",
    "serialization_df.insert(3, 'manhwa_published', value=None)\n",
    "serialization_df.insert(4, 'owner', value=None)\n",
    "serialization_df.insert(5, 'launched', value=None)\n",
    "\n",
    "serialization_df.to_csv('..\\data\\serialization.csv', index=False)\n",
    "\n",
    "manhwa_info_df = manhwa_info_df.merge(serialization_df, on='serialization', how='left')\n",
    "manhwa_info_df = manhwa_info_df.drop(['serialization', 'website', 'manhwa_published', 'owner', 'launched'], axis=1)\n",
    "\n",
    "manhwa_info_df.to_csv('..\\data\\manhwa_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning remove null and duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/manhwa_genre.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../data/manhwa_genre.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('../data/manhwa_author.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../data/manhwa_author.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
